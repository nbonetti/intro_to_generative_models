{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5467d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import random \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BONETTI\\Desktop\\3A\\Intro to generative models\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du tokenizer...\n",
      "Téléchargement du modèle...\n",
      "Modèle chargé sur cpu !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# On choisit un modèle très léger (environ 500 Mo)\n",
    "model_name = \"facebook/opt-125m\" \n",
    "#model_name = \"Qwen/Qwen2.5-1.5B\" # Modèle de base de l'article\n",
    "\n",
    "print(\"Téléchargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Téléchargement du modèle...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# On déplace le modèle sur le GPU si disponible, sinon CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Modèle chargé sur {device} !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c5f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_log_likelihood(model, tokenizer, sequence):\n",
    "    \"\"\"Calcule la log-vraisemblance log(p(x)) d'une séquence.\"\"\"\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # Le modèle renvoie déjà la CrossEntropy (Negative Log Likelihood moyenne)\n",
    "        # On la multiplie par le nombre de tokens pour avoir la somme des log-probs\n",
    "        log_p_x = -outputs.loss.item() * (input_ids.shape[1] - 1)\n",
    "        \n",
    "    return log_p_x, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524436e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Départ: The most important number in math is | Log P: -24.77\n",
      "Étape 1: REJETÉ | Nouveau texte: The most important number in the world is the numb... | Log P: -37.95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_correction_tracker(model, tokenizer, initial_text, alpha=15.0, steps=10, block_size=15):\n",
    "    current_text = initial_text\n",
    "    # Calcul initial\n",
    "    current_log_p, _ = compute_log_likelihood(model, tokenizer, current_text)\n",
    "    \n",
    "    history = []\n",
    "    history.append({\"step\": 0, \"text\": current_text, \"log_p\": current_log_p, \"status\": \"Initial\"})\n",
    "\n",
    "    print(f\"Départ: {current_text} | Log P: {current_log_p:.2f}\")\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        # 1. On transforme le texte en IDs pour manipuler les blocs\n",
    "        input_ids = tokenizer.encode(current_text, return_tensors=\"pt\").to(model.device)\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        # 2. Choisir un bloc à modifier (on évite le tout début)\n",
    "        start_idx = random.randint(min(5, seq_len-1), max(5, seq_len - block_size - 1))\n",
    "        prefix_ids = input_ids[:, :start_idx]\n",
    "\n",
    "        # 3. Proposer un nouveau bloc (x')\n",
    "        with torch.no_grad():\n",
    "            new_block_ids = model.generate(\n",
    "                prefix_ids, \n",
    "                max_new_tokens=block_size, \n",
    "                do_sample=True, \n",
    "                temperature=1.0, # On échantillonne normalement\n",
    "                attention_mask=prefix_ids.ne(tokenizer.pad_token_id).long(),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        proposed_text = tokenizer.decode(new_block_ids[0], skip_special_tokens=True)\n",
    "        #print(f\"Phrase complète à cette étape : \\n {proposed_text}\")\n",
    "        proposed_log_p, _ = compute_log_likelihood(model, tokenizer, proposed_text)\n",
    "\n",
    "        # 4. Calcul du ratio Metropolis-Hastings\n",
    "        # log(A) = alpha * (log_p_proposed - log_p_current)\n",
    "        acceptance_log_ratio = alpha * (proposed_log_p - current_log_p)\n",
    "        \n",
    "        accepted = False\n",
    "        if np.log(random.random()) < acceptance_log_ratio:\n",
    "            current_text = proposed_text\n",
    "            current_log_p = proposed_log_p\n",
    "            accepted = True\n",
    "\n",
    "        status = \"ACCEPTÉ\" if accepted else \"REJETÉ\"\n",
    "        print(f\"Étape {i}: {status} | Nouveau texte: {proposed_text[:50]}... | Log P: {proposed_log_p:.2f}\")\n",
    "        \n",
    "        history.append({\n",
    "            \"step\": i, \n",
    "            \"text\": proposed_text, \n",
    "            \"log_p\": proposed_log_p, \n",
    "            \"status\": status,\n",
    "            \"final_text_at_step\": current_text\n",
    "        })\n",
    "\n",
    "    return history\n",
    "\n",
    "# --- TEST ---\n",
    "# On commence volontairement avec une phrase un peu bancale\n",
    "prompt_initial = \"The most important number in math is\"\n",
    "tracker_results = run_correction_tracker(model, tokenizer, prompt_initial, alpha=16, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Départ: The most important number in math is | Log P: -26.93\n",
      "Étape 1: REJETÉ | Nouveau texte: The most important number of Americans are the one... | Log P: -30.82\n",
      "Étape 2: REJETÉ | Nouveau texte: The most important number of the weekend\n",
      "Today... | Log P: -37.02\n",
      "Étape 3: REJETÉ | Nouveau texte: The most important number on the wall is \"... | Log P: -32.15\n",
      "Étape 4: REJETÉ | Nouveau texte: The most important number for which I know to... | Log P: -35.60\n",
      "Étape 5: REJETÉ | Nouveau texte: The most important number, is between 4-... | Log P: -35.69\n",
      "Étape 6: REJETÉ | Nouveau texte: The most important number is what percentage of wo... | Log P: -32.69\n",
      "Étape 7: REJETÉ | Nouveau texte: The most important number of new jobs to be... | Log P: -33.68\n",
      "Étape 8: REJETÉ | Nouveau texte: The most important number of people who have used... | Log P: -30.63\n",
      "Étape 9: REJETÉ | Nouveau texte: The most important number in the case is a... | Log P: -31.89\n",
      "Étape 10: REJETÉ | Nouveau texte: The most important number of students getting paid... | Log P: -37.51\n",
      "Départ: The most important number in math is | Log P: -26.93\n",
      "Étape 1: REJETÉ | Nouveau texte: The most important number for an A2A... | Log P: -36.91\n",
      "Étape 2: REJETÉ | Nouveau texte: The most important number in the world to me... | Log P: -28.13\n",
      "Étape 3: REJETÉ | Nouveau texte: The most important number (not necessary) in... | Log P: -36.01\n",
      "Étape 4: REJETÉ | Nouveau texte: The most important number on the list is probably... | Log P: -28.25\n",
      "Étape 5: REJETÉ | Nouveau texte: The most important number is the number of calorie... | Log P: -27.92\n",
      "Étape 6: REJETÉ | Nouveau texte: The most important number would be the length of... | Log P: -30.02\n",
      "Étape 7: REJETÉ | Nouveau texte: The most important number to know in the past... | Log P: -33.72\n",
      "Étape 8: REJETÉ | Nouveau texte: The most important number, even if not the... | Log P: -31.73\n",
      "Étape 9: REJETÉ | Nouveau texte: The most important number in that area!\n",
      "... | Log P: -36.88\n",
      "Étape 10: REJETÉ | Nouveau texte: The most important number is one on a small... | Log P: -36.75\n",
      "Départ: The most important number in math is | Log P: -26.93\n",
      "Étape 1: REJETÉ | Nouveau texte: The most important number for me to be aware... | Log P: -32.70\n",
      "Étape 2: REJETÉ | Nouveau texte: The most important number of Americans in the coun... | Log P: -31.81\n",
      "Étape 3: REJETÉ | Nouveau texte: The most important number is the minimum.\n",
      "... | Log P: -30.09\n",
      "Étape 4: REJETÉ | Nouveau texte: The most important number is 0 when in the... | Log P: -33.18\n",
      "Étape 5: ACCEPTÉ | Nouveau texte: The most important number is the length of the... | Log P: -26.85\n",
      "Étape 6: REJETÉ | Nouveau texte: The most important number of hours that one can... | Log P: -35.25\n",
      "Étape 7: REJETÉ | Nouveau texte: The most important number to consider when startin... | Log P: -33.03\n",
      "Étape 8: REJETÉ | Nouveau texte: The most important number to use is \"N... | Log P: -33.61\n",
      "Étape 9: ACCEPTÉ | Nouveau texte: The most important number for me is that I... | Log P: -26.02\n",
      "Étape 10: ACCEPTÉ | Nouveau texte: The most important number in the world is the... | Log P: -25.93\n",
      "Départ: The most important number in math is | Log P: -26.93\n",
      "Étape 1: REJETÉ | Nouveau texte: The most important number is the total, I... | Log P: -33.50\n",
      "Étape 2: REJETÉ | Nouveau texte: The most important number to look out for during... | Log P: -29.00\n",
      "Étape 3: REJETÉ | Nouveau texte: The most important number for everyone is 18.... | Log P: -33.75\n",
      "Étape 4: REJETÉ | Nouveau texte: The most important number of games I played was... | Log P: -29.58\n",
      "Étape 5: REJETÉ | Nouveau texte: The most important number is actually the distance... | Log P: -33.87\n",
      "Étape 6: REJETÉ | Nouveau texte: The most important number we'll see so far... | Log P: -34.24\n",
      "Étape 7: REJETÉ | Nouveau texte: The most important number two in my book is... | Log P: -30.61\n",
      "Étape 8: REJETÉ | Nouveau texte: The most important number with all the up votes... | Log P: -37.91\n",
      "Étape 9: REJETÉ | Nouveau texte: The most important number are the times (and... | Log P: -37.68\n",
      "Étape 10: REJETÉ | Nouveau texte: The most important number is 10.\n",
      "*... | Log P: -31.20\n",
      "Départ: The most important number in math is | Log P: -26.93\n",
      "Étape 1: REJETÉ | Nouveau texte: The most important number for each element is call... | Log P: -36.27\n",
      "Étape 2: REJETÉ | Nouveau texte: The most important number in your life. That... | Log P: -32.51\n",
      "Étape 3: REJETÉ | Nouveau texte: The most important number of countries in the U... | Log P: -32.68\n",
      "Étape 4: REJETÉ | Nouveau texte: The most important number in the word-number... | Log P: -37.08\n",
      "Étape 5: REJETÉ | Nouveau texte: The most important number in the article is that... | Log P: -28.33\n",
      "Étape 6: ACCEPTÉ | Nouveau texte: The most important number of people in the world... | Log P: -24.12\n",
      "Étape 7: REJETÉ | Nouveau texte: The most important number in the universe is the... | Log P: -28.38\n",
      "Étape 8: REJETÉ | Nouveau texte: The most important number is 0.\n",
      "It... | Log P: -28.16\n",
      "Étape 9: REJETÉ | Nouveau texte: The most important number of countries are the oth... | Log P: -35.02\n",
      "Étape 10: REJETÉ | Nouveau texte: The most important number for the most important w... | Log P: -35.89\n",
      "\n",
      "--- RÉSULTATS ---\n",
      "GREEDY: The most important number in math is the number of times you have to use the word \"I\" to make a statement.\n",
      "I | LogP: -58.78\n",
      "POWER : The most important number in the world is the | LogP: -25.93\n"
     ]
    }
   ],
   "source": [
    "def compare_greedy_vs_power(model, tokenizer, prompt, alpha=15.0):\n",
    "    # 1. Génération Greedy (Standard)\n",
    "    greedy_output = model.generate(\n",
    "        tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device),\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False # Mode Greedy\n",
    "    )\n",
    "    greedy_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "    greedy_log_p, _ = compute_log_likelihood(model, tokenizer, greedy_text)\n",
    "\n",
    "    # 2. Génération Power Sampling (On prend le meilleur de 5 chaînes MCMC)\n",
    "    power_results = []\n",
    "    for i in range(5):\n",
    "        # On lance un MCMC court\n",
    "        trace = run_correction_tracker(model, tokenizer, prompt, alpha=alpha, steps=10, block_size=5)\n",
    "        final_version = trace[-1]\n",
    "        power_results.append(final_version)\n",
    "\n",
    "    # Trier pour trouver le meilleur résultat Power Sampling\n",
    "    best_power = max(power_results, key=lambda x: x['log_p'])\n",
    "\n",
    "    print(\"\\n--- RÉSULTATS ---\")\n",
    "    print(f\"GREEDY: {greedy_text} | LogP: {greedy_log_p:.2f}\")\n",
    "    print(f\"POWER : {best_power['final_text_at_step']} | LogP: {best_power['log_p']:.2f}\")\n",
    "    \n",
    "    return greedy_log_p, best_power['log_p']\n",
    "\n",
    "# Test\n",
    "g_score, p_score = compare_greedy_vs_power(model, tokenizer, \"The most important number in math is\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
