{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# On choisit un modèle très léger (environ 500 Mo)\n",
    "model_name = \"facebook/opt-125m\" \n",
    "\n",
    "print(\"Téléchargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Téléchargement du modèle...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# On déplace le modèle sur le GPU si disponible, sinon CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Modèle chargé sur {device} !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "def compute_log_likelihood(model, tokenizer, sequence):\n",
    "    \"\"\"Calcule la log-vraisemblance log(p(x)) d'une séquence.\"\"\"\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # Le modèle renvoie déjà la CrossEntropy (Negative Log Likelihood moyenne)\n",
    "        # On la multiplie par le nombre de tokens pour avoir la somme des log-probs\n",
    "        log_p_x = -outputs.loss.item() * (input_ids.shape[1] - 1)\n",
    "        \n",
    "    return log_p_x, inputs\n",
    "\n",
    "def run_correction_tracker(model, tokenizer, initial_text, alpha=16.0, steps=20, block_size=15):\n",
    "    current_text = initial_text\n",
    "    # Calcul initial\n",
    "    current_log_p, _ = compute_log_likelihood(model, tokenizer, current_text)\n",
    "    \n",
    "    history = []\n",
    "    history.append({\"step\": 0, \"text\": current_text, \"log_p\": current_log_p, \"status\": \"Initial\"})\n",
    "\n",
    "    print(f\"Départ: {current_text} | Log P: {current_log_p:.2f}\")\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        # 1. On transforme le texte en IDs pour manipuler les blocs\n",
    "        input_ids = tokenizer.encode(current_text, return_tensors=\"pt\").to(model.device)\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        # 2. Choisir un bloc à modifier (on évite le tout début)\n",
    "        start_idx = random.randint(min(5, seq_len-1), max(5, seq_len - block_size - 1))\n",
    "        prefix_ids = input_ids[:, :start_idx]\n",
    "\n",
    "        # 3. Proposer un nouveau bloc (x')\n",
    "        with torch.no_grad():\n",
    "            new_block_ids = model.generate(\n",
    "                prefix_ids, \n",
    "                max_new_tokens=block_size, \n",
    "                do_sample=True, \n",
    "                temperature=1.0, # On échantillonne normalement\n",
    "                attention_mask=prefix_ids.ne(tokenizer.pad_token_id).long(),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        proposed_text = tokenizer.decode(new_block_ids[0], skip_special_tokens=True)\n",
    "        #print(f\"Phrase complète à cette étape : \\n {proposed_text}\")\n",
    "        proposed_log_p, _ = compute_log_likelihood(model, tokenizer, proposed_text)\n",
    "\n",
    "        # 4. Calcul du ratio Metropolis-Hastings\n",
    "        # log(A) = alpha * (log_p_proposed - log_p_current)\n",
    "        acceptance_log_ratio = alpha * (proposed_log_p - current_log_p)\n",
    "        \n",
    "        accepted = False\n",
    "        if np.log(random.random()) < acceptance_log_ratio:\n",
    "            current_text = proposed_text\n",
    "            current_log_p = proposed_log_p\n",
    "            accepted = True\n",
    "\n",
    "        status = \"ACCEPTÉ\" if accepted else \"REJETÉ\"\n",
    "        print(f\"Étape {i}: {status} | Nouveau texte: {proposed_text[:50]}... | Log P: {proposed_log_p:.2f}\")\n",
    "        \n",
    "        history.append({\n",
    "            \"step\": i, \n",
    "            \"text\": proposed_text, \n",
    "            \"log_p\": proposed_log_p, \n",
    "            \"status\": status,\n",
    "            \"final_text_at_step\": current_text\n",
    "        })\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST ---\n",
    "# On commence volontairement avec une phrase un peu bancale\n",
    "prompt_initial = \"A prime number is a number that has three divisors: 1, itself and\"\n",
    "tracker_results = run_correction_tracker(model, tokenizer, prompt_initial, alpha=16, steps=20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
