{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314b443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BONETTI\\Desktop\\3A\\Intro to generative models\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du tokenizer...\n",
      "Téléchargement du modèle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé sur cpu !\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# On choisit un modèle très léger (environ 500 Mo)\n",
    "model_name = \"facebook/opt-125m\" \n",
    "\n",
    "print(\"Téléchargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Téléchargement du modèle...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# On déplace le modèle sur le GPU si disponible, sinon CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(f\"Modèle chargé sur {device} !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ce5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "def compute_log_likelihood(model, tokenizer, sequence):\n",
    "    #Calcule la log-vraisemblance log(p(x)) d'une séquence\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # Le modèle renvoie déjà la CrossEntropy (Negative Log Likelihood moyenne)\n",
    "        # On la multiplie par le nombre de tokens pour avoir la somme des log-probs\n",
    "        log_p_x = -outputs.loss.item() * (input_ids.shape[1] - 1)\n",
    "        \n",
    "    return log_p_x, inputs\n",
    "\n",
    "def run_correction_tracker(model, tokenizer, initial_text, alpha=16.0, steps=20, block_size=15):\n",
    "    current_text = initial_text\n",
    "    # Calcul initial\n",
    "    current_log_p, _ = compute_log_likelihood(model, tokenizer, current_text)\n",
    "    \n",
    "    history = []\n",
    "    history.append({\"step\": 0, \"text\": current_text, \"log_p\": current_log_p, \"status\": \"Initial\"})\n",
    "\n",
    "    print(f\"Départ: {current_text} | Log P: {current_log_p:.2f}\")\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        # 1. On transforme le texte en IDs pour manipuler les blocs\n",
    "        input_ids = tokenizer.encode(current_text, return_tensors=\"pt\").to(model.device)\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        # 2. Choisir un bloc à modifier (on évite le tout début)\n",
    "        start_idx = random.randint(min(5, seq_len-1), max(5, seq_len - block_size - 1))\n",
    "        prefix_ids = input_ids[:, :start_idx]\n",
    "\n",
    "        # 3. Proposer un nouveau bloc (x')\n",
    "        with torch.no_grad():\n",
    "            new_block_ids = model.generate(\n",
    "                prefix_ids, \n",
    "                max_new_tokens=block_size, \n",
    "                do_sample=True, \n",
    "                temperature=1.0, # On échantillonne normalement\n",
    "                attention_mask=prefix_ids.ne(tokenizer.pad_token_id).long(),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        proposed_text = tokenizer.decode(new_block_ids[0], skip_special_tokens=True)\n",
    "        #print(f\"Phrase complète à cette étape : \\n {proposed_text}\")\n",
    "        proposed_log_p, _ = compute_log_likelihood(model, tokenizer, proposed_text)\n",
    "\n",
    "        # 4. Calcul du ratio Metropolis-Hastings\n",
    "        # log(A) = alpha * (log_p_proposed - log_p_current)\n",
    "        acceptance_log_ratio = alpha * (proposed_log_p - current_log_p)\n",
    "        \n",
    "        accepted = False\n",
    "        if np.log(random.random()) < acceptance_log_ratio:\n",
    "            current_text = proposed_text\n",
    "            current_log_p = proposed_log_p\n",
    "            accepted = True\n",
    "\n",
    "        status = \"ACCEPTÉ\" if accepted else \"REJETÉ\"\n",
    "        print(f\"Étape {i}: {status} | Nouveau texte: {proposed_text[:50]}... | Log P: {proposed_log_p:.2f}\")\n",
    "        \n",
    "        history.append({\n",
    "            \"step\": i, \n",
    "            \"text\": proposed_text, \n",
    "            \"log_p\": proposed_log_p, \n",
    "            \"status\": status,\n",
    "            \"final_text_at_step\": current_text\n",
    "        })\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd38c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Départ: A prime number is a number that has three divisors: 1, itself and | Log P: -66.02\n",
      "Étape 1: ACCEPTÉ | Nouveau texte: A prime number is typically a prime number. For ex... | Log P: -54.62\n",
      "Étape 2: REJETÉ | Nouveau texte: A prime number is a good number for when to pick u... | Log P: -64.50\n",
      "Étape 3: REJETÉ | Nouveau texte: A prime number is a very short answer to a simple ... | Log P: -56.24\n",
      "Étape 4: ACCEPTÉ | Nouveau texte: A prime number is 2 divided by 1.\n",
      "2\n",
      "Calculate the ... | Log P: -35.80\n",
      "Étape 5: REJETÉ | Nouveau texte: A prime number is the key to a great man. The worl... | Log P: -67.01\n",
      "Étape 6: REJETÉ | Nouveau texte: A prime number is a number used to denote a number... | Log P: -55.44\n",
      "Étape 7: REJETÉ | Nouveau texte: A prime number is 1.999999.\n",
      "There is no prime numb... | Log P: -60.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BONETTI\\Desktop\\3A\\Intro to generative models\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BONETTI\\.cache\\huggingface\\hub\\models--facebook--opt-125m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Étape 8: REJETÉ | Nouveau texte: A prime number is a number given to an arrangement... | Log P: -60.13\n",
      "Étape 9: REJETÉ | Nouveau texte: A prime number is 3.\n",
      "It's the number I am willing ... | Log P: -64.04\n",
      "Étape 10: REJETÉ | Nouveau texte: A prime number is 5*(1 - 6/(-19)). Suppose 2*x =... | Log P: -53.00\n",
      "Étape 11: REJETÉ | Nouveau texte: A prime number is the number of people you see a w... | Log P: -53.66\n",
      "Étape 12: REJETÉ | Nouveau texte: A prime number is used with confidence when descri... | Log P: -74.34\n",
      "Étape 13: REJETÉ | Nouveau texte: A prime number is a real number, and therefore it'... | Log P: -62.02\n",
      "Étape 14: REJETÉ | Nouveau texte: A prime number is just a few hundred digits.\n",
      ">A pr... | Log P: -43.47\n",
      "Étape 15: REJETÉ | Nouveau texte: A prime number is one set of numbers of digits, li... | Log P: -61.40\n",
      "Étape 16: REJETÉ | Nouveau texte: A prime number is a small number of the number one... | Log P: -65.80\n",
      "Étape 17: REJETÉ | Nouveau texte: A prime number is a number, not a prime number, th... | Log P: -51.15\n",
      "Étape 18: REJETÉ | Nouveau texte: A prime number is a single digit number that may b... | Log P: -55.06\n",
      "Étape 19: REJETÉ | Nouveau texte: A prime number is 6-6, unless its the most common,... | Log P: -72.60\n",
      "Étape 20: REJETÉ | Nouveau texte: A prime number is that. It's also how many times y... | Log P: -66.01\n"
     ]
    }
   ],
   "source": [
    "# --- TEST ---\n",
    "# On commence volontairement avec une phrase un peu bancale\n",
    "prompt_initial = \"A prime number is a number that has three divisors: 1, itself and\"\n",
    "tracker_results = run_correction_tracker(model, tokenizer, prompt_initial, alpha=16, steps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
